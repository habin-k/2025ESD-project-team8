import os
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence
from sklearn.metrics import accuracy_score, f1_score
import matplotlib.pyplot as plt
import json
import random
from tqdm.notebook import tqdm  # Colab í™˜ê²½ì—ì„œ tqdm ì‚¬ìš©

# âœ… CUDA ì„¤ì •
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"âœ… Using device: {device}")

# 1. ëª¨ë“  Training í´ë” ë‚´ .npz íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸° (ì¬ê·€ íƒìƒ‰)
def load_npz_in_training_folders(root_dir):
    sequences, labels, lengths = [], [], []
    found_files = 0
    
    for subdir, _, files in os.walk(root_dir):
        if 'Training' in subdir.split(os.sep):
            for fname in tqdm(files, desc="ğŸ” Loading npz files", leave=False):
                if fname.endswith(".npz"):
                    npz_path = os.path.join(subdir, fname)
                    
                    try:
                        npz_data = np.load(npz_path, allow_pickle=True)
                        if "pose" in npz_data.files and "label" in npz_data.files:
                            data = npz_data["pose"]
                            label = npz_data["label"]
                            
                            if data.ndim == 3 and data.shape[1:] == (17, 2):
                                data = data.reshape(data.shape[0], -1)
                                zero_mask = (data == 0).all(axis=1)
                                for t in range(1, data.shape[0]):
                                    if zero_mask[t]:
                                        data[t] = data[t-1]

                                sequences.append(torch.tensor(data, dtype=torch.float32))
                                labels.append(torch.tensor(label, dtype=torch.float32))
                                lengths.append(data.shape[0])
                                found_files += 1

                    except Exception as e:
                        print(f"íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {npz_path}, ì˜¤ë¥˜: {e}")

    print(f"âœ… ì´ {found_files}ê°œì˜ ë°ì´í„° ë¡œë”© ì™„ë£Œ")
    return sequences, labels, lengths

# 2. ê²½ë¡œ ì„¤ì •
fall_dir = "/content/drive/MyDrive/pose_tensor_npz/Y/Training"
normal_dir = "/content/drive/MyDrive/pose_tensor_npz/N/Training"

fall_seqs, fall_labs, fall_lens = load_npz_in_training_folders(fall_dir)
normal_seqs, normal_labs, normal_lens = load_npz_in_training_folders(normal_dir)

# 3. ë°ì´í„° ê²°í•© ë° ì…”í”Œ
sequences = fall_seqs + normal_seqs
labels = fall_labs + normal_labs
lengths = fall_lens + normal_lens
combined = list(zip(sequences, labels, lengths))
random.shuffle(combined)
sequences, labels, lengths = zip(*combined)

# 4. Dataset ì •ì˜
class FallDataset(Dataset):
    def __init__(self, sequences, labels, lengths):
        self.sequences = sequences
        self.labels = labels
        self.lengths = torch.tensor(lengths, dtype=torch.long)

    def __len__(self):
        return len(self.sequences)

    def __getitem__(self, idx):
        return self.sequences[idx], self.labels[idx], self.lengths[idx]

# 5. Collate í•¨ìˆ˜ (íŒ¨ë”©)
def collate_fn(batch):
    sequences, labels, lengths = zip(*batch)
    padded_seq = pad_sequence(sequences, batch_first=True)
    padded_labels = pad_sequence(labels, batch_first=True)
    return padded_seq, padded_labels, torch.tensor(lengths)

# 6. DataLoader ì¤€ë¹„
dataset = FallDataset(sequences, labels, lengths)
loader = DataLoader(dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)

# 7. LSTM ëª¨ë¸ ì •ì˜ (í”„ë ˆì„ë³„ ì˜ˆì¸¡)
class PackedLSTMClassifier(nn.Module):
    def __init__(self, input_size=34, hidden_size=128):
        super().__init__()
        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True).to(device)
        self.fc = nn.Linear(hidden_size, 1).to(device)
        self.sigmoid = nn.Sigmoid().to(device)

    def forward(self, x, lengths):
        packed = pack_padded_sequence(x, lengths.cpu(), batch_first=True, enforce_sorted=False)
        packed_output, (hn, _) = self.lstm(packed)
        unpacked_output, _ = torch.nn.utils.rnn.pad_packed_sequence(packed_output, batch_first=True)
        
        # ê° í”„ë ˆì„ë§ˆë‹¤ ì˜ˆì¸¡
        out = self.sigmoid(self.fc(unpacked_output))  # (Batch, Sequence Length, 1)
        return out.squeeze(-1)  # (Batch, Sequence Length)

# ëª¨ë¸ ì´ˆê¸°í™”
model = PackedLSTMClassifier().to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
loss_fn = nn.BCELoss()
loss_history = []

# 8. í•™ìŠµ ë£¨í”„ (í”„ë ˆì„ë³„ ì˜ˆì¸¡)
for epoch in range(10):
    all_preds, all_labels = [], []
    model.train()
    
    with tqdm(loader, desc=f"ğŸ”§ Training Epoch {epoch+1}", leave=False) as pbar:
        for x, y, lengths in pbar:
            x, y = x.to(device), y.to(device)
            pred = model(x, lengths)  # (Batch, Sequence Length)
            
            # ì†ì‹¤ ê³„ì‚° (í”„ë ˆì„ë³„ Binary Cross Entropy)
            loss = loss_fn(pred, y)
            loss.backward()
            optimizer.step()
            optimizer.zero_grad()
            loss_history.append(loss.item())

            all_preds += (pred > 0.5).int().flatten().tolist()
            all_labels += y.int().flatten().tolist()
            pbar.set_postfix({"Loss": loss.item()})

    # ì •í™•ë„ì™€ F1 ì ìˆ˜ ê³„ì‚° (í”„ë ˆì„ ë‹¨ìœ„)
    acc = accuracy_score(all_labels, all_preds)
    f1 = f1_score(all_labels, all_preds)
    print(f"âœ… Epoch {epoch+1} | Loss: {loss.item():.4f} | Acc: {acc:.4f} | F1: {f1:.4f}")

# 9. í•™ìŠµ ê²°ê³¼ ì €ì¥
save_dir = "/content/drive/MyDrive/lstm_training_results"
torch.save(model.state_dict(), os.path.join(save_dir, "best_lstm_model.pth"))

results = {
    "loss_history": loss_history,
    "final_accuracy": acc,
    "final_f1_score": f1
}

with open(os.path.join(save_dir, "training_results.json"), "w") as f:
    json.dump(results, f)

print(f"âœ… í•™ìŠµ ê²°ê³¼ ì €ì¥ ì™„ë£Œ.")
